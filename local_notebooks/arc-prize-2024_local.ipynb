{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d1e38-c37b-4d33-8590-a77c58e4edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Daniel Franzen and Jan Disselhoff\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a390b-52b6-4ba1-9c2a-f553d4f096ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook contains our winning submission to the ARC Prize 2024 Kaggle competition,\n",
    "# scoring 53.5 points on the private evaluation set.\n",
    "# the ARChitects (Daniel Franzen and Jan Disselhoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3064736",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T16:42:52.203810Z",
     "iopub.status.busy": "2024-11-09T16:42:52.203518Z",
     "iopub.status.idle": "2024-11-09T16:44:49.747146Z",
     "shell.execute_reply": "2024-11-09T16:44:49.746001Z"
    },
    "papermill": {
     "duration": 117.552978,
     "end_time": "2024-11-09T16:44:49.749311",
     "exception": false,
     "start_time": "2024-11-09T16:42:52.196333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# THIS IS IF WE WANT TO RUN ON MORE THAN ONE GPU\n",
    "NUM_GPUS=1\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "if NUM_GPUS>1:\n",
    "    from common_stuff import *\n",
    "    if not os.path.exists(os.path.join(tmp_dir, 'unsloth_installed')):  # unsloth offline install - https://stackoverflow.com/a/51646354\n",
    "        !pip uninstall --yes torch accelerate\n",
    "        !pip install --no-index --find-links=/kaggle/input/unsloth-2024-9-post4/wheelhouse unsloth\n",
    "        #!pip uninstall --yes accelerate fastai torch torchaudio transformers\n",
    "        #!pip install --no-index --find-links=/kaggle/input/unsloth-2024-10-7/wheelhouse unsloth  # do not use grad_acc_fix - trains very slow\n",
    "        #!sed -i 's/if ((post_check - pre_check) >= 1).sum() > 1:/if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\n",
    "        # fix delay bug in get_statistics()\n",
    "        !sed -i 's/^def get_statistics():/def get_statistics():\\n if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/_utils.py\n",
    "        # fix faulty unsloth multi-gpu detection\n",
    "        !sed -i \"s/raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')/pass/g\" /opt/conda/lib/python3.10/site-packages/unsloth/tokenizer_utils.py /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py /opt/conda/lib/python3.10/site-packages/unsloth/models/vision.py\n",
    "        os.makedirs(os.path.join(tmp_dir, 'unsloth_installed'), exist_ok=True)\n",
    "        print('Unsloth installed & patched.')\n",
    "\n",
    "    for gpu in [0, 1]: \n",
    "        signal_path = f'{model_temp_storage}_gpu{gpu}_done'\n",
    "        if os.path.exists(signal_path): os.rmdir(signal_path)\n",
    "\n",
    "    if arc_test_set.is_fake:  # cleanup? (for debugging)\n",
    "        #!rm -R /kaggle/temp/finetuned_model*\n",
    "        #!rm -R /kaggle/temp/inference_outputs\n",
    "        #!rm -R /kaggle/temp/inference_scoring\n",
    "        #!ls /kaggle/temp\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a55247e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T16:44:49.770848Z",
     "iopub.status.busy": "2024-11-09T16:44:49.770097Z",
     "iopub.status.idle": "2024-11-09T16:44:49.783934Z",
     "shell.execute_reply": "2024-11-09T16:44:49.783093Z"
    },
    "papermill": {
     "duration": 0.026738,
     "end_time": "2024-11-09T16:44:49.785908",
     "exception": false,
     "start_time": "2024-11-09T16:44:49.759170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load challanges from '/home/nikolai/Desktop/Uni/1_software/arc-prize-2024-winning-solution/local_notebooks/../data/arc-prize-2024/arc-agi_evaluation_challenges.json'...\n",
      "====================\n",
      "\n",
      "We are running: False\n",
      "*** Load base model and tokenizer from './temp/da-fr--Mistral-NeMo-Minitron-8B-ARChitects-Full-bnb-4bit/transformers/default/1'...\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolai/Desktop/Uni/1_software/arc-prize-2024-winning-solution/env_arc24_winning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.1.6: Fast Mistral patching. Transformers: 4.48.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.669 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "*** Create new peft model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.6 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n",
      "Note we are skipping augmentation\n",
      "*** Reducing task size to max. 4224 tokens...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 444.03it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:00<00:00, 1401.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Start training run...\n",
      "*** WARNING: using faulty unsloth gradient accumulation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 200 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 4 | Total steps = 50\n",
      " \"-____-\"     Number of trainable parameters = 188,723,200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 06:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.039400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** -> Training took 385.6371 seconds.\n",
      "*** Saving model/tokenizer to './temp/finetuned_model_gpu0'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolai/Desktop/Uni/1_software/arc-prize-2024-winning-solution/env_arc24_winning/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** GPU: NVIDIA GeForce RTX 3090, used 9.46 / 23.7 GB.\n"
     ]
    }
   ],
   "source": [
    "# If using the --bg flag this will execute asynchronously \n",
    "# (you will then need to uncomment the await cell)\n",
    "# %%python --bg --proc train_proc0\n",
    "from common_stuff import *\n",
    "start_training(gpu=0, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba84dcea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T16:44:49.807420Z",
     "iopub.status.busy": "2024-11-09T16:44:49.806758Z",
     "iopub.status.idle": "2024-11-09T16:44:49.814102Z",
     "shell.execute_reply": "2024-11-09T16:44:49.813234Z"
    },
    "papermill": {
     "duration": 0.021089,
     "end_time": "2024-11-09T16:44:49.816838",
     "exception": false,
     "start_time": "2024-11-09T16:44:49.795749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%python --bg --proc train_proc1\n",
    "# from common_stuff import *\n",
    "# start_training(gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd2e275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T16:44:49.838580Z",
     "iopub.status.busy": "2024-11-09T16:44:49.837779Z",
     "iopub.status.idle": "2024-11-09T16:44:49.846418Z",
     "shell.execute_reply": "2024-11-09T16:44:49.845418Z"
    },
    "papermill": {
     "duration": 0.022037,
     "end_time": "2024-11-09T16:44:49.849235",
     "exception": false,
     "start_time": "2024-11-09T16:44:49.827198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Load challanges from '/home/nikolai/Desktop/Uni/1_software/arc-prize-2024-winning-solution/local_notebooks/../data/arc-prize-2024/arc-agi_evaluation_challenges.json'...\n",
      "*** Load base model and tokenizer from './temp/finetuned_model_gpu0'...\n",
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolai/Desktop/Uni/1_software/arc-prize-2024-winning-solution/env_arc24_winning/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.1.6: Fast Mistral patching. Transformers: 4.48.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.669 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.6 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note we are skipping augmentation\n",
      "*** Reducing task size to max. 8192 tokens (7260 input + 932 generated)...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 208/208 [00:00<00:00, 513.52it/s] \n",
      "*** Load stored data...\n",
      "*** Start inference run...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [54:39<00:00, 16.40s/it]\n",
      "*** Completed inference run.\n",
      "calculate augmented scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 202/202 [20:39<00:00,  6.14s/it]\n",
      "*** GPU: NVIDIA GeForce RTX 3090, used 7.28 / 23.7 GB.\n"
     ]
    }
   ],
   "source": [
    "# %%python --bg --proc infer_proc0\n",
    "from common_stuff import *\n",
    "start_inference(gpu=0, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc3521a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T16:44:49.880731Z",
     "iopub.status.busy": "2024-11-09T16:44:49.880347Z",
     "iopub.status.idle": "2024-11-09T16:44:49.895884Z",
     "shell.execute_reply": "2024-11-09T16:44:49.894944Z"
    },
    "papermill": {
     "duration": 0.034338,
     "end_time": "2024-11-09T16:44:49.898618",
     "exception": false,
     "start_time": "2024-11-09T16:44:49.864280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%python --bg --proc infer_proc1\n",
    "# from common_stuff import *\n",
    "# start_inference(gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af29b14c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T16:44:49.960470Z",
     "iopub.status.busy": "2024-11-09T16:44:49.959960Z",
     "iopub.status.idle": "2024-11-09T16:57:34.353608Z",
     "shell.execute_reply": "2024-11-09T16:57:34.352332Z"
    },
    "papermill": {
     "duration": 764.430937,
     "end_time": "2024-11-09T16:57:34.355778",
     "exception": false,
     "start_time": "2024-11-09T16:44:49.924841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is only needed if we launch the pythcon process as python --bg\n",
    "# proc_exit_codes = await wait_for_subprocesses(train_proc0, train_proc1, infer_proc0, infer_proc1, print_output=True or arc_test_set.is_fake)\n",
    "# print(f'*** Subprocesses exit codes: {proc_exit_codes}')\n",
    "# assert all(x==0 for x in proc_exit_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11bbec36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-09T16:57:34.524539Z",
     "iopub.status.busy": "2024-11-09T16:57:34.523594Z",
     "iopub.status.idle": "2024-11-09T16:57:34.980317Z",
     "shell.execute_reply": "2024-11-09T16:57:34.979214Z"
    },
    "papermill": {
     "duration": 0.544378,
     "end_time": "2024-11-09T16:57:34.982314",
     "exception": false,
     "start_time": "2024-11-09T16:57:34.437936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate augmented scores: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 202/202 [00:01<00:00, 154.56it/s]\n",
      "*** Generating submission for 202 outputs...\n"
     ]
    }
   ],
   "source": [
    "# write submission\n",
    "from common_stuff import *\n",
    "with RemapCudaOOM():\n",
    "    model, formatter, dataset = None, MyFormatter(), None\n",
    "    decoder = Decoder(formatter, arc_test_set.split_multi_replies(), n_guesses=2, frac_score=True).from_store(infer_params['store'])\n",
    "    if use_aug_score or arc_test_set.is_fake: decoder.calc_augmented_scores(model=model, store=score_temp_storage, **aug_score_params)\n",
    "    submission = arc_test_set.get_submission(decoder.run_selection_algo(submission_select_algo))\n",
    "    with open('submission.json', 'w') as f: json.dump(submission, f)\n",
    "    if arc_test_set.is_fake:\n",
    "        decoder.benchmark_selection_algos(selection_algorithms)\n",
    "        with open('submission.json') as f: reload_submission = json.load(f)\n",
    "        print('*** Reload score:', arc_test_set.validate_submission(reload_submission))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8951125,
     "sourceId": 67357,
     "sourceType": "competition"
    },
    {
     "datasetId": 5793177,
     "sourceId": 9515958,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 157175,
     "modelInstanceId": 134422,
     "sourceId": 158171,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env_arc24_winning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 886.241405,
   "end_time": "2024-11-09T16:57:35.525139",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-09T16:42:49.283734",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
